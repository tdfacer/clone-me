# Backend: Python Scripts

## 1. `clone_me_qa_generator.py`

### Purpose
This script generates a user persona, answers training questions using an LLM, and outputs the data as a CSV file for fine-tuning.

### Usage
```bash
# Navigate to the python directory
cd python

# Set up a virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Run the script
python clone_me_qa_generator.py
```

### Output
- **JSON Persona**: Saved to `./data/output/persona_<persona_name>.json`.
- **Answered Questions CSV**: Saved to `./data/output/qa_output_<persona_name>.csv`.

---

## 2. `upload_dataset_to_hub.py`

### Purpose
Uploads the dataset generated by the QA script to the Hugging Face Hub.

### Usage
```bash
python upload_dataset_to_hub.py "<path_to_output_from_qa_script>" "<your_hugging_face_username>/<your_dataset_name>"
```

---

# Data Structure

### Input Data
- **Example Question Dataset**: `./data/input/extended_questionnaire.csv`

### Output Data
- **User Persona JSON**: Contains the persona generated by the LLM.
- **Question & Answer CSV**: Contains the responses generated by the LLM.

---

# Fine-Tuning: Jupyter Notebook

## Location
- `./python/notebooks`

## Overview
The notebook demonstrates fine-tuning the `DeepSeek-R1-Distill-Llama-8B` model using:
- A training dataset created with the `clone_me_qa_generator.py` script.
- The Unsloth library for faster training and lower memory usage.

